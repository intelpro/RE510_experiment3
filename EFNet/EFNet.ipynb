{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/intelpro/TA_session\n",
    "!ls ./TA_session/EFNet/\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import로 모듈 가져오기\n",
    "torch에 있는 module, 우리가 사전에 정의해놓았던 함수 등 import로 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-Image fusion module 정의를 위해서 여러가지 함수들을 import 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch의 dimension을 rearrange 하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_3d(x):\n",
    "    return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer normalization 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasFree_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(BiasFree_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "class WithBias_LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape):\n",
    "        super(WithBias_LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "        assert len(normalized_shape) == 1\n",
    "\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.normalized_shape = normalized_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = x.mean(-1, keepdim=True)\n",
    "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, LayerNorm_type):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if LayerNorm_type =='BiasFree':\n",
    "            self.body = BiasFree_LayerNorm(dim)\n",
    "        else:\n",
    "            self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        return to_4d(self.body(to_3d(x)), h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event-Image fusion module 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutual_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, bias):\n",
    "        super(Mutual_Attention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.k = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.v = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        assert x.shape == y.shape, 'The shape of feature maps from image and event branch are not equal!'\n",
    "\n",
    "        b,c,h,w = x.shape\n",
    "\n",
    "        q = self.q(x) # image\n",
    "        k = self.k(y) # event\n",
    "        v = self.v(y) # event\n",
    "        \n",
    "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v)\n",
    "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
    "        out = self.project_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "## Event-Image Channel Attention (EICA)\n",
    "class EventImage_ChannelAttentionTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ffn_expansion_factor=2, bias=False, LayerNorm_type='WithBias'):\n",
    "        super(EventImage_ChannelAttentionTransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1_image = LayerNorm(dim, LayerNorm_type)\n",
    "        self.norm1_event = LayerNorm(dim, LayerNorm_type)\n",
    "        self.attn = Mutual_Attention(dim, num_heads, bias)\n",
    "        # mlp\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * ffn_expansion_factor)\n",
    "        self.ffn = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=nn.GELU, drop=0.)\n",
    "\n",
    "    def forward(self, image, event):\n",
    "        # image: b, c, h, w\n",
    "        # event: b, c, h, w\n",
    "        # return: b, c, h, w\n",
    "        assert image.shape == event.shape, 'the shape of image doesnt equal to event'\n",
    "        b, c , h, w = image.shape\n",
    "        fused = image + self.attn(self.norm1_image(image), self.norm1_event(event)) # b, c, h, w\n",
    "\n",
    "        # mlp\n",
    "        fused = to_3d(fused) # b, h*w, c\n",
    "        fused = fused + self.ffn(self.norm2(fused))\n",
    "        fused = to_4d(fused, h, w)\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_chn, out_chn, bias=True):\n",
    "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=3, stride=1, padding=1, bias=bias)\n",
    "    return layer\n",
    "\n",
    "def conv_down(in_chn, out_chn, bias=False):\n",
    "    layer = nn.Conv2d(in_chn, out_chn, kernel_size=4, stride=2, padding=1, bias=bias)\n",
    "    return layer\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):\n",
    "    return nn.Conv2d(\n",
    "        in_channels, out_channels, kernel_size,\n",
    "        padding=(kernel_size//2), bias=bias, stride = stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downsample, relu_slope, use_emgc=False, num_heads=None): # cat\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
    "        self.use_emgc = use_emgc\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)\n",
    "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)        \n",
    "\n",
    "        if downsample and use_emgc:\n",
    "            self.emgc_enc = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_dec = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_enc_mask = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_dec_mask = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "\n",
    "        if downsample:\n",
    "            self.downsample = conv_down(out_size, out_size, bias=False)\n",
    "\n",
    "        if self.num_heads is not None:\n",
    "            self.image_event_transformer = EventImage_ChannelAttentionTransformerBlock(out_size, num_heads=self.num_heads, ffn_expansion_factor=4, bias=False, LayerNorm_type='WithBias')\n",
    "        \n",
    "\n",
    "    def forward(self, x, enc=None, dec=None, mask=None, event_filter=None, merge_before_downsample=True):\n",
    "        out = self.conv_1(x)\n",
    "\n",
    "        out_conv1 = self.relu_1(out)\n",
    "        out_conv2 = self.relu_2(self.conv_2(out_conv1))\n",
    "\n",
    "        out = out_conv2 + self.identity(x)\n",
    "\n",
    "        if enc is not None and dec is not None and mask is not None:\n",
    "            assert self.use_emgc\n",
    "            out_enc = self.emgc_enc(enc) + self.emgc_enc_mask((1-mask)*enc)\n",
    "            out_dec = self.emgc_dec(dec) + self.emgc_dec_mask(mask*dec)\n",
    "            out = out + out_enc + out_dec        \n",
    "            \n",
    "        if event_filter is not None and merge_before_downsample:\n",
    "            # b, c, h, w = out.shape\n",
    "            out = self.image_event_transformer(out, event_filter) \n",
    "             \n",
    "        if self.downsample:\n",
    "            out_down = self.downsample(out)\n",
    "            if not merge_before_downsample: \n",
    "                out_down = self.image_event_transformer(out_down, event_filter) \n",
    "\n",
    "            return out_down, out\n",
    "\n",
    "        else:\n",
    "            if merge_before_downsample:\n",
    "                return out\n",
    "            else:\n",
    "                out = self.image_event_transformer(out, event_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEVConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, downsample, relu_slope, use_emgc=False):\n",
    "        super(UNetEVConvBlock, self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.identity = nn.Conv2d(in_size, out_size, 1, 1, 0)\n",
    "        self.use_emgc = use_emgc\n",
    "        self.conv_1 = nn.Conv2d(in_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_1 = nn.LeakyReLU(relu_slope, inplace=False)\n",
    "        self.conv_2 = nn.Conv2d(out_size, out_size, kernel_size=3, padding=1, bias=True)\n",
    "        self.relu_2 = nn.LeakyReLU(relu_slope, inplace=False)\n",
    "\n",
    "        self.conv_before_merge = nn.Conv2d(out_size, out_size , 1, 1, 0) \n",
    "        if downsample and use_emgc:\n",
    "            self.emgc_enc = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_dec = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_enc_mask = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "            self.emgc_dec_mask = nn.Conv2d(out_size, out_size, 3, 1, 1)\n",
    "\n",
    "        if downsample:\n",
    "            self.downsample = conv_down(out_size, out_size, bias=False)\n",
    "\n",
    "    def forward(self, x, merge_before_downsample=True):\n",
    "        out = self.conv_1(x)\n",
    "        out_conv1 = self.relu_1(out)\n",
    "        out_conv2 = self.relu_2(self.conv_2(out_conv1))\n",
    "        out = out_conv2 + self.identity(x)\n",
    "             \n",
    "        if self.downsample:\n",
    "            out_down = self.downsample(out)\n",
    "            if not merge_before_downsample: \n",
    "            \n",
    "                out_down = self.conv_before_merge(out_down)\n",
    "            else : \n",
    "                out = self.conv_before_merge(out)\n",
    "            return out_down, out\n",
    "        else:\n",
    "\n",
    "            out = self.conv_before_merge(out)\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, relu_slope):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2, bias=True)\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, False, relu_slope)\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        out = torch.cat([up, bridge], 1)\n",
    "        out = self.conv_block(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EF-Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LAB]SAM(Supervised attention module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supervised Attention Module\n",
    "## https://github.com/swz30/MPRNet\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, n_feat, kernel_size=3, bias=True):\n",
    "        super(SAM, self).__init__()\n",
    "        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n",
    "        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n",
    "        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n",
    "\n",
    "    def forward(self, x, x_img):\n",
    "        x1 = NotImplemented\n",
    "        img = NotImplemented\n",
    "        return x1, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EFNet(nn.Module):\n",
    "    def __init__(self, in_chn=3, ev_chn=6, wf=64, depth=3, fuse_before_downsample=True, relu_slope=0.2, num_heads=[1,2,4]):\n",
    "        super(EFNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.fuse_before_downsample = fuse_before_downsample\n",
    "        self.num_heads = num_heads\n",
    "        self.down_path_1 = nn.ModuleList()\n",
    "        self.down_path_2 = nn.ModuleList()\n",
    "        self.conv_01 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
    "        self.conv_02 = nn.Conv2d(in_chn, wf, 3, 1, 1)\n",
    "        # event\n",
    "        self.down_path_ev = nn.ModuleList()\n",
    "        self.conv_ev1 = nn.Conv2d(ev_chn, wf, 3, 1, 1)\n",
    "        prev_channels = self.get_input_chn(wf)\n",
    "        for i in range(depth):\n",
    "            downsample = True if (i+1) < depth else False \n",
    "\n",
    "            self.down_path_1.append(UNetConvBlock(prev_channels, (2**i) * wf, downsample, relu_slope, num_heads=self.num_heads[i]))\n",
    "            self.down_path_2.append(UNetConvBlock(prev_channels, (2**i) * wf, downsample, relu_slope, use_emgc=downsample))\n",
    "            # ev encoder\n",
    "            if i < self.depth:\n",
    "                self.down_path_ev.append(UNetEVConvBlock(prev_channels, (2**i) * wf, downsample , relu_slope))\n",
    "\n",
    "            prev_channels = (2**i) * wf\n",
    "\n",
    "        self.up_path_1 = nn.ModuleList()\n",
    "        self.up_path_2 = nn.ModuleList()\n",
    "        self.skip_conv_1 = nn.ModuleList()\n",
    "        self.skip_conv_2 = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path_1.append(UNetUpBlock(prev_channels, (2**i)*wf, relu_slope))\n",
    "            self.up_path_2.append(UNetUpBlock(prev_channels, (2**i)*wf, relu_slope))\n",
    "            self.skip_conv_1.append(nn.Conv2d((2**i)*wf, (2**i)*wf, 3, 1, 1))\n",
    "            self.skip_conv_2.append(nn.Conv2d((2**i)*wf, (2**i)*wf, 3, 1, 1))\n",
    "            prev_channels = (2**i)*wf\n",
    "        self.sam12 = SAM(prev_channels)\n",
    "\n",
    "        self.cat12 = nn.Conv2d(prev_channels*2, prev_channels, 1, 1, 0)\n",
    "        self.last = conv3x3(prev_channels, in_chn, bias=True)\n",
    "\n",
    "    def forward(self, x, event, mask=None):\n",
    "        image = x\n",
    "\n",
    "        ev = []\n",
    "        #EVencoder\n",
    "        e1 = self.conv_ev1(event)\n",
    "        for i, down in enumerate(self.down_path_ev):\n",
    "            if i < self.depth-1:\n",
    "                e1, e1_up = down(e1, self.fuse_before_downsample)\n",
    "                if self.fuse_before_downsample:\n",
    "                    ev.append(e1_up)\n",
    "                else:\n",
    "                    ev.append(e1)\n",
    "            else:\n",
    "                e1 = down(e1, self.fuse_before_downsample)\n",
    "                ev.append(e1)\n",
    "\n",
    "        #stage 1\n",
    "        x1 = self.conv_01(image)\n",
    "        encs = []\n",
    "        decs = []\n",
    "        masks = []\n",
    "        for i, down in enumerate(self.down_path_1):\n",
    "            if (i+1) < self.depth:\n",
    "\n",
    "                x1, x1_up = down(x1, event_filter=ev[i], merge_before_downsample=self.fuse_before_downsample)\n",
    "                encs.append(x1_up)\n",
    "\n",
    "                if mask is not None:\n",
    "                    masks.append(F.interpolate(mask, scale_factor = 0.5**i))\n",
    "            \n",
    "            else:\n",
    "                x1 = down(x1, event_filter=ev[i], merge_before_downsample=self.fuse_before_downsample)\n",
    "\n",
    "\n",
    "        for i, up in enumerate(self.up_path_1):\n",
    "            x1 = up(x1, self.skip_conv_1[i](encs[-i-1]))\n",
    "            decs.append(x1)\n",
    "        sam_feature, out_1 = self.sam12(x1, image)\n",
    "\n",
    "        #stage 2\n",
    "        x2 = self.conv_02(image)\n",
    "        x2 = self.cat12(torch.cat([x2, sam_feature], dim=1))\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path_2):\n",
    "            if (i+1) < self.depth:\n",
    "                if mask is not None:\n",
    "                    x2, x2_up = down(x2, encs[i], decs[-i-1], mask=masks[i])\n",
    "                else:\n",
    "                    x2, x2_up = down(x2, encs[i], decs[-i-1])\n",
    "                blocks.append(x2_up)\n",
    "            else:\n",
    "                x2 = down(x2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path_2):\n",
    "            x2 = up(x2, self.skip_conv_2[i](blocks[-i-1]))\n",
    "\n",
    "        out_2 = self.last(x2)\n",
    "        out_2 = out_2 + image\n",
    "\n",
    "        return [out_1, out_2]\n",
    "\n",
    "    def get_input_chn(self, in_chn):\n",
    "        return in_chn\n",
    "\n",
    "    def _initialize(self):\n",
    "        gain = nn.init.calculate_gain('leaky_relu', 0.20)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.orthogonal_(m.weight, gain=gain)\n",
    "                if not m.bias is None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
